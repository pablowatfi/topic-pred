{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /Users/pablowatfi/repos/topic-pred/.venv/bin/python\n",
      "pip: 25.3\n",
      "numpy: 1.26.4\n",
      "pandas: 2.3.3\n",
      "sklearn: 1.7.2\n",
      "sentence_transformers: 5.1.2\n",
      "torch: 2.2.2\n",
      "transformers: 4.57.1\n",
      "joblib: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "# Environment verification: prints interpreter and key package versions\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "\n",
    "pkgs = [\n",
    "    \"pip\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"sklearn\",\n",
    "    \"sentence_transformers\",\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"joblib\",\n",
    "]\n",
    "\n",
    "for p in pkgs:\n",
    "    try:\n",
    "        m = importlib.import_module(p)\n",
    "        v = getattr(m, \"__version__\", \"(no __version__)\")\n",
    "        print(f\"{p}: {v}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{p}: not installed ({e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-to-Topic Prediction Model\n",
    "\n",
    "## Overview\n",
    "This notebook implements a content-to-topic matching system using:\n",
    "1. **Sentence Transformers** for multilingual embeddings\n",
    "2. **Direct Topic Prediction** (no intermediate clustering - predicts actual topics)\n",
    "3. **Embedding-based Similarity** for semantic matching\n",
    "\n",
    "## Key Features\n",
    "- **Content Sampling**: Process a subset of content for faster testing\n",
    "- **Direct Predictions**: Predict actual topic IDs (no intermediate clustering)\n",
    "- **Embedding-only Scoring**: Uses sentence-transformers embeddings for similarity\n",
    "- **Multilingual Support**: Works across different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import joblib\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Sample content: True\n",
      "  Sample size: 20000\n",
      "  Model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "  Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ========================\n",
    "\n",
    "# Sampling settings\n",
    "SAMPLE_CONTENT = True  # Set to False to use all content\n",
    "CONTENT_SAMPLE_SIZE = 20000  # Number of content items to sample\n",
    "\n",
    "# Model settings\n",
    "SENTENCE_TRANSFORMER_MODEL = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Paths\n",
    "ROOT = Path.cwd().parent\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "ARTIFACTS_DIR = ROOT / \"artifacts\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Sample content: {SAMPLE_CONTENT}\")\n",
    "if SAMPLE_CONTENT:\n",
    "    print(f\"  Sample size: {CONTENT_SAMPLE_SIZE}\")\n",
    "print(f\"  Model: {SENTENCE_TRANSFORMER_MODEL}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: (154047, 8)\n",
      "Topics: (76972, 9)\n",
      "Correlations: (61517, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load data files\n",
    "content_path = DATA_DIR / \"content.csv\"\n",
    "correlations_path = DATA_DIR / \"correlations.csv\"\n",
    "topics_path = DATA_DIR / \"topics.csv\"\n",
    "\n",
    "content_df = pd.read_csv(content_path)\n",
    "topics_df = pd.read_csv(topics_path)\n",
    "correlations_df = pd.read_csv(correlations_path)\n",
    "\n",
    "print(f\"Content: {content_df.shape}\")\n",
    "print(f\"Topics: {topics_df.shape}\")\n",
    "print(f\"Correlations: {correlations_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content Sample:\n",
      "               id                                             title  \\\n",
      "0  c_00002381196d  Sumar números de varios dígitos: 48,029+233,930    \n",
      "1  c_000087304a9e                    Trovare i fattori di un numero   \n",
      "2  c_0000ad142ddb                           Sumar curvas de demanda   \n",
      "\n",
      "                                         description   kind text language  \\\n",
      "0  Suma 48,029+233,930 mediante el algoritmo está...  video  NaN       es   \n",
      "1                    Sal trova i fattori di 120.\\n\\n  video  NaN       it   \n",
      "2                  Cómo añadir curvas de demanda\\n\\n  video  NaN       es   \n",
      "\n",
      "  copyright_holder license  \n",
      "0              NaN     NaN  \n",
      "1              NaN     NaN  \n",
      "2              NaN     NaN  \n",
      "\n",
      "Topics Sample:\n",
      "               id                                   title  \\\n",
      "0  t_00004da3a1b2              Откриването на резисторите   \n",
      "1  t_000095e03056  Unit 3.3 Enlargements and Similarities   \n",
      "2  t_00068291e9a4         Entradas e saídas de uma função   \n",
      "\n",
      "                                         description channel category  level  \\\n",
      "0  Изследване на материали, които предизвикват на...  000cf7   source      4   \n",
      "1                                                NaN  b3f329  aligned      2   \n",
      "2               Entenda um pouco mais sobre funções.  8e286a   source      4   \n",
      "\n",
      "  language          parent  has_content  \n",
      "0       bg  t_16e29365b50d         True  \n",
      "1       en  t_aa32fb6252dc        False  \n",
      "2       pt  t_d14b6c2a2b70         True  \n",
      "\n",
      "Correlations Sample:\n",
      "         topic_id                                        content_ids\n",
      "0  t_00004da3a1b2  c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c...\n",
      "1  t_00068291e9a4  c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c...\n",
      "2  t_00069b63a70a                                     c_11a1dc0bfb99\n"
     ]
    }
   ],
   "source": [
    "# Preview data\n",
    "print(\"\\nContent Sample:\")\n",
    "print(content_df.head(3))\n",
    "print(\"\\nTopics Sample:\")\n",
    "print(topics_df.head(3))\n",
    "print(\"\\nCorrelations Sample:\")\n",
    "print(correlations_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Process Topics (All Topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all topics...\n",
      "Processed 76972 topics\n",
      "\n",
      "Topic examples:\n",
      "\n",
      "Topic 1: Откриването на резисторите\n",
      "\n",
      "Topic 2: Unit 3.3 Enlargements and Similarities\n",
      "\n",
      "Topic 3: Entradas e saídas de uma função\n"
     ]
    }
   ],
   "source": [
    "# Process Topics (All Topics)\n",
    "\n",
    "# Create combined text for topics\n",
    "print(\"Processing all topics...\")\n",
    "topics_df['combined_text'] = (\n",
    "    topics_df['title'].fillna('') + ' ' +\n",
    "    topics_df['description'].fillna('')\n",
    ").str.strip()\n",
    "\n",
    "print(f\"Processed {len(topics_df)} topics\")\n",
    "\n",
    "# Create topic ID to index mapping\n",
    "topic_id_to_idx = {topic_id: idx for idx, topic_id in enumerate(topics_df['id'])}\n",
    "topic_idx_to_id = {idx: topic_id for topic_id, idx in topic_id_to_idx.items()}\n",
    "\n",
    "print(\"\\nTopic examples:\")\n",
    "for i in range(min(3, len(topics_df))):\n",
    "    print(f\"\\nTopic {i+1}: {topics_df.iloc[i]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Content (with Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 20000 out of 154047 content items...\n",
      "Sampled content shape: (20000, 8)\n",
      "\n",
      "Processing content...\n",
      "Processed 20000 content items\n",
      "\n",
      "Content examples:\n",
      "\n",
      "Content 1:\n",
      "  Text: आधे और चौथाई Practice dividing shapes into 2 or 4 equal sections....\n",
      "\n",
      "Content 2:\n",
      "  Text: Lateral Inhibition Which is in control, your eye or your brain?...\n",
      "\n",
      "Content 3:\n",
      "  Text: TI-AIE: Conjecturing and generalising in mathematics: introducing algebra...\n",
      "Sampled content shape: (20000, 8)\n",
      "\n",
      "Processing content...\n",
      "Processed 20000 content items\n",
      "\n",
      "Content examples:\n",
      "\n",
      "Content 1:\n",
      "  Text: आधे और चौथाई Practice dividing shapes into 2 or 4 equal sections....\n",
      "\n",
      "Content 2:\n",
      "  Text: Lateral Inhibition Which is in control, your eye or your brain?...\n",
      "\n",
      "Content 3:\n",
      "  Text: TI-AIE: Conjecturing and generalising in mathematics: introducing algebra...\n"
     ]
    }
   ],
   "source": [
    "# Sample content if enabled\n",
    "if SAMPLE_CONTENT and len(content_df) > CONTENT_SAMPLE_SIZE:\n",
    "    print(f\"Sampling {CONTENT_SAMPLE_SIZE} out of {len(content_df)} content items...\")\n",
    "    sampled_indices = np.random.choice(len(content_df), CONTENT_SAMPLE_SIZE, replace=False)\n",
    "    sampled_indices = sorted(sampled_indices)\n",
    "    content_df_sampled = content_df.iloc[sampled_indices].reset_index(drop=True)\n",
    "    print(f\"Sampled content shape: {content_df_sampled.shape}\")\n",
    "else:\n",
    "    print(\"Using all content (no sampling)\")\n",
    "    content_df_sampled = content_df.copy()\n",
    "    sampled_indices = np.arange(len(content_df))\n",
    "\n",
    "# Create combined text\n",
    "print(\"\\nProcessing content...\")\n",
    "content_df_sampled['combined_text'] = (\n",
    "    content_df_sampled['title'].fillna('') + ' ' +\n",
    "    content_df_sampled['description'].fillna('')\n",
    ").str.strip()\n",
    "\n",
    "print(f\"Processed {len(content_df_sampled)} content items\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nContent examples:\")\n",
    "for i in range(min(3, len(content_df_sampled))):\n",
    "    print(f\"\\nContent {i+1}:\")\n",
    "    print(f\"  Text: {content_df_sampled.iloc[i]['combined_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Content-Topic Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-topic mappings: 154047\n",
      "Total content items with topics: 154047\n",
      "Average topics per content: 1.82\n",
      "\n",
      "Sampled content with topics: 20000/20000\n"
     ]
    }
   ],
   "source": [
    "# Parse correlations\n",
    "correlations_df['content_list'] = correlations_df['content_ids'].str.split()\n",
    "\n",
    "content_to_topics = defaultdict(list)\n",
    "for _, row in correlations_df.iterrows():\n",
    "    topic_id = row['topic_id']\n",
    "    for content_id in row['content_list']:\n",
    "        content_to_topics[content_id].append(topic_id)\n",
    "\n",
    "print(f\"Content-topic mappings: {len(content_to_topics)}\")\n",
    "\n",
    "print(f\"Total content items with topics: {len(content_to_topics)}\")\n",
    "print(f\"Average topics per content: {np.mean([len(v) for v in content_to_topics.values()]):.2f}\")\n",
    "\n",
    "# Count how many sampled content items have topic associations\n",
    "sampled_with_topics = sum(1 for cid in content_df_sampled['id'] if cid in content_to_topics)\n",
    "print(f\"\\nSampled content with topics: {sampled_with_topics}/{len(content_df_sampled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load multilingual sentence transformer\n",
    "print(\"Loading sentence transformer model...\")\n",
    "model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating topic embeddings (all topics)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1203/1203 [02:54<00:00,  6.89it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic embeddings shape: (76972, 384)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for ALL topics (no sampling - we need all topics for predictions)\n",
    "print(\"\\nGenerating topic embeddings (all topics)...\")\n",
    "topic_texts = topics_df['combined_text'].tolist()\n",
    "topic_embeddings = model.encode(\n",
    "    topic_texts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(f\"Topic embeddings shape: {topic_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating content embeddings (sampled)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.06it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.06it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.06it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.06it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.11it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.11it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.48it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.48it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.00it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.00it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.51it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.51it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.24it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.24it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.44it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.44it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.12it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.12it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.26it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.26it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.05it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.05it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.95it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.95it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.89it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.89it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.84it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.84it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.97it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.97it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.44it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.44it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.99it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.99it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.75it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.75it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.85it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.85it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.62it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.62it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.90it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.90it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.22it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.22it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.54it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.54it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.94it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.94it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.66it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.66it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.57it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.57it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.94it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.94it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 14.03it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 14.03it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.99it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.99it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.49it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.49it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.35it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.35it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.58it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.58it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.23it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.23it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.84it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.84it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.75it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.75it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.85it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.85it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.65it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.65it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.68it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.68it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.61it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.61it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.12it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.12it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.61it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.61it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.16it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.16it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.52it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.52it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.07it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.07it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.62it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.62it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.97it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.97it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.91it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.91it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.67it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.67it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.27it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.27it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.27it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.27it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.25it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.25it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.61it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.61it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.10it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.10it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.67it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.67it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.39it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.39it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.78it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.78it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.02it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.02it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.26it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.26it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.30it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.30it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.35it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.35it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.35it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.35it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.61it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.61it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.53it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.53it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.20it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.20it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.36it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.36it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.16it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.16it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.62it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.62it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.92it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.92it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.01it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.01it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.36it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.36it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.34it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.34it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.89it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.89it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.19it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.19it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.11it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.11it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.09it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.09it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.49it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.49it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.33it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.33it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.99it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.99it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.03it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.03it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.94it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.94it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.34it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.34it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.56it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.07it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.07it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.00it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.00it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.85it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.85it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.23it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.23it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.23it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.23it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.93it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.93it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.03it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.03it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.37it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.37it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.87it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.28it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.28it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.20it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.20it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.58it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.58it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.02it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.02it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.62it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.62it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.97it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.97it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.34it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.34it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.58it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.58it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.59it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.59it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.77it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.77it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.70it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.70it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.28it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.28it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.05it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.05it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.81it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.51it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.51it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.08it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.08it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.98it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.98it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 12.19it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 12.19it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.77it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.77it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.93it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.93it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.65it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.65it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.17it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.17it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.66it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.66it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.77it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.77it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.26it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.26it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.49it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.49it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.19it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.19it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.50it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.35it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.35it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.67it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.67it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.58it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.58it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.79it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.79it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.13it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.13it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.24it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.24it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.37it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.37it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.15it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.15it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.59it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.59it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.09it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.09it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.68it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.68it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.67it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.67it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.11it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.11it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.39it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.39it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.94it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.94it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.79it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.96it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content embeddings shape: (20000, 384)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for sampled content\n",
    "# we are going to predict only on these content items to see the metrics\n",
    "print(\"\\nGenerating content embeddings (sampled)...\")\n",
    "content_embeddings = []\n",
    "\n",
    "for i in range(0, len(content_df_sampled), BATCH_SIZE):\n",
    "    batch = content_df_sampled['combined_text'].iloc[i:i+BATCH_SIZE].tolist()\n",
    "    batch_embeddings = model.encode(batch, show_progress_bar=True)\n",
    "    content_embeddings.extend(batch_embeddings)\n",
    "\n",
    "    if (i + BATCH_SIZE) % 5000 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "content_embeddings = np.array(content_embeddings)\n",
    "print(f\"Content embeddings shape: {content_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7:  Prediction Function\n",
    "\n",
    "This function predicts topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topics(\n",
    "    content_embedding: np.ndarray,\n",
    "    topic_embeddings: np.ndarray,\n",
    "    topic_ids: List[str],\n",
    "    min_score: float = 0.3,\n",
    "    top_k: int = 3\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Predict topics using embedding similarity only.\n",
    "\n",
    "    Returns up to `top_k` topic ids whose cosine similarity with the content\n",
    "    embedding is >= min_score.\n",
    "    \"\"\"\n",
    "    # Calculate embedding similarities\n",
    "    embedding_similarities = cosine_similarity(\n",
    "        content_embedding.reshape(1, -1),\n",
    "        topic_embeddings\n",
    "    )[0]\n",
    "\n",
    "    # Use embeddings-only scores\n",
    "    hybrid_scores = embedding_similarities\n",
    "\n",
    "    # Get top-k topics above threshold\n",
    "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "    predicted_topics = [\n",
    "        topic_ids[idx] for idx in top_indices\n",
    "        if hybrid_scores[idx] >= min_score\n",
    "    ]\n",
    "\n",
    "    return predicted_topics\n",
    "\n",
    "\n",
    "def predict_topics_batch(\n",
    "    content_embeddings: np.ndarray,\n",
    "    topic_embeddings: np.ndarray,\n",
    "    topic_ids: List[str],\n",
    "    min_score: float = 0.3,\n",
    "    top_k: int = 3\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Predict topics for MULTIPLE contents at once (batch processing) using embeddings only.\n",
    "    \"\"\"\n",
    "    # Calculate ALL embedding similarities at once (n_contents x n_topics)\n",
    "    embedding_similarities = cosine_similarity(\n",
    "        content_embeddings,\n",
    "        topic_embeddings\n",
    "    )\n",
    "\n",
    "    # Combine scores (embeddings only)\n",
    "    hybrid_scores = embedding_similarities\n",
    "\n",
    "    # Get top-k predictions for each content\n",
    "    predictions = []\n",
    "    for i in range(len(content_embeddings)):\n",
    "        top_indices = np.argsort(hybrid_scores[i])[::-1][:top_k]\n",
    "        pred_topics = [\n",
    "            topic_ids[idx] for idx in top_indices\n",
    "            if hybrid_scores[i, idx] >= min_score\n",
    "        ]\n",
    "        predictions.append(pred_topics)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Training samples: 20000\n",
      "Average topics per sample: 1.80\n",
      "Total unique topics in training: 23680\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data from sampled content\n",
    "print(\"Preparing training data...\")\n",
    "train_indices = []\n",
    "y_train = []  # Will contain lists of topic IDs\n",
    "\n",
    "for idx, content_id in enumerate(content_df_sampled['id']):\n",
    "    if content_id in content_to_topics:\n",
    "        train_indices.append(idx)\n",
    "        topic_ids = content_to_topics[content_id]\n",
    "        y_train.append(topic_ids)\n",
    "\n",
    "X_train = content_embeddings[train_indices]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Average topics per sample: {np.mean([len(topics) for topics in y_train]):.2f}\")\n",
    "print(f\"Total unique topics in training: {len(set([t for topics in y_train for t in topics]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total topics available for prediction: 76972\n"
     ]
    }
   ],
   "source": [
    "# Prepare topic data for prediction function\n",
    "topic_ids_list = topics_df['id'].tolist()\n",
    "\n",
    "print(f\"Total topics available for prediction: {len(topic_ids_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation sample size: 500\n"
     ]
    }
   ],
   "source": [
    "# Sample for faster evaluation (optional)\n",
    "EVAL_SAMPLE_SIZE = min(500, len(X_train))\n",
    "eval_indices = np.random.choice(len(X_train), EVAL_SAMPLE_SIZE, replace=False)\n",
    "\n",
    "X_eval = X_train[eval_indices]\n",
    "y_eval = [y_train[i] for i in eval_indices]\n",
    "\n",
    "print(f\"Evaluation sample size: {EVAL_SAMPLE_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Parameter Grid Evaluation\n",
      "================================================================================\n",
      "\n",
      "Testing: top_k=2, min_score=0.3\n",
      "\n",
      "Testing: top_k=3, min_score=0.3\n",
      "\n",
      "Testing: top_k=3, min_score=0.3\n",
      "\n",
      "Testing: top_k=5, min_score=0.3\n",
      "\n",
      "Testing: top_k=5, min_score=0.3\n",
      "\n",
      "================================================================================\n",
      "GRID EVALUATION RESULTS (Using Embedding Similarity >= 0.90)\n",
      "================================================================================\n",
      " top_k  min_score  F1_Score  Precision  Recall  Avg_Pred_Topics  Exact_Match_%\n",
      "     2     0.3000    0.2048     0.1960  0.2144           2.0000        14.0000\n",
      "     3     0.3000    0.2063     0.1660  0.2724           3.0000        15.0000\n",
      "     5     0.3000    0.1998     0.1364  0.3731           5.0000        15.6000\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GRID EVALUATION RESULTS (Using Embedding Similarity >= 0.90)\n",
      "================================================================================\n",
      " top_k  min_score  F1_Score  Precision  Recall  Avg_Pred_Topics  Exact_Match_%\n",
      "     2     0.3000    0.2048     0.1960  0.2144           2.0000        14.0000\n",
      "     3     0.3000    0.2063     0.1660  0.2724           3.0000        15.0000\n",
      "     5     0.3000    0.1998     0.1364  0.3731           5.0000        15.6000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Grid search parameters\n",
    "top_k_values = [2, 3, 5]\n",
    "min_score_values = [0.3]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Parameter Grid Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store results for all configurations\n",
    "all_results = []\n",
    "\n",
    "# Helper function to find matching topics based on embedding similarity\n",
    "def find_matching_topics(predicted_topics, true_topics, similarity_threshold=0.90):\n",
    "    \"\"\"\n",
    "    Find matching topics between predicted and true topics based on embedding similarity.\n",
    "    Returns sets of matched and unmatched predicted topics.\n",
    "    \"\"\"\n",
    "    # Get texts for predicted and true topics\n",
    "    pred_texts = [topics_df[topics_df['id'] == tid]['combined_text'].iloc[0] for tid in predicted_topics]\n",
    "    true_texts = [topics_df[topics_df['id'] == tid]['combined_text'].iloc[0] for tid in true_topics]\n",
    "\n",
    "    # Convert texts to embeddings (this is not efficient at all, but works for now)\n",
    "    pred_embeddings = model.encode(pred_texts)\n",
    "    true_embeddings = model.encode(true_texts)\n",
    "\n",
    "    # Calculate similarity matrix\n",
    "    similarities = cosine_similarity(pred_embeddings, true_embeddings)\n",
    "\n",
    "    # Find matches\n",
    "    matched_pred = set()\n",
    "    unmatched_pred = set()\n",
    "\n",
    "    for i, pred_tid in enumerate(predicted_topics):\n",
    "        # Check if this prediction matches any true topic with high similarity\n",
    "        max_sim = similarities[i].max() if len(similarities[i]) > 0 else 0\n",
    "        if max_sim >= similarity_threshold:\n",
    "            matched_pred.add(pred_tid)\n",
    "        else:\n",
    "            unmatched_pred.add(pred_tid)\n",
    "\n",
    "    return matched_pred, unmatched_pred\n",
    "\n",
    "# Test each combination of parameters\n",
    "for top_k in top_k_values:\n",
    "    for min_score in min_score_values:\n",
    "        print(f\"\\nTesting: top_k={top_k}, min_score={min_score:.1f}\")\n",
    "\n",
    "        # Make predictions with current parameters\n",
    "        y_pred = predict_topics_batch(\n",
    "            X_eval,\n",
    "            topic_embeddings,\n",
    "            topic_ids_list,\n",
    "            min_score=min_score,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Calculate metrics using embedding similarity\n",
    "        tp = 0  # True positives\n",
    "        fp = 0  # False positives\n",
    "        fn = 0  # False negatives\n",
    "\n",
    "        for pred_topics, true_topics in zip(y_pred, y_eval):\n",
    "            # Find matches based on embedding similarity\n",
    "            matched_pred, unmatched_pred = find_matching_topics(pred_topics, true_topics)\n",
    "\n",
    "            # Update metrics\n",
    "            tp += len(matched_pred)\n",
    "            fp += len(unmatched_pred)\n",
    "            # Count true topics that don't have a matching prediction\n",
    "            fn += len(true_topics) - len(matched_pred)\n",
    "\n",
    "        # Calculate final metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Calculate prediction stats\n",
    "        avg_pred = np.mean([len(topics) for topics in y_pred])\n",
    "        exact_matches = 0\n",
    "        for pred, true in zip(y_pred, y_eval):\n",
    "            matched, _ = find_matching_topics(pred, true)\n",
    "            if len(matched) == len(true):  # All true topics have matches\n",
    "                exact_matches += 1\n",
    "        exact_match_pct = (exact_matches / len(y_eval)) * 100\n",
    "\n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'top_k': top_k,\n",
    "            'min_score': min_score,\n",
    "            'F1_Score': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Avg_Pred_Topics': avg_pred,\n",
    "            'Exact_Match_%': exact_match_pct\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values(['top_k', 'min_score'])\n",
    "\n",
    "# Format the display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRID EVALUATION RESULTS (Using Embedding Similarity >= 0.90)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Detailed Evaluation with Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get topic details\n",
    "def get_topic_details(topic_ids, max_display=3):\n",
    "    \"\"\"Display topic titles and descriptions.\"\"\"\n",
    "    for tid in topic_ids[:max_display]:\n",
    "        topic_row = topics_df[topics_df['id'] == tid]\n",
    "        if not topic_row.empty:\n",
    "            title = topic_row.iloc[0]['title']\n",
    "            desc = str(topic_row.iloc[0]['description'])[:100]\n",
    "            print(f\"    - [{tid}] {title}: {desc}\")\n",
    "    if len(topic_ids) > max_display:\n",
    "        print(f\"    ... and {len(topic_ids) - max_display} more topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running detailed evaluation on 5000 samples...\n",
      "\n",
      "================================================================================\n",
      "FINAL EVALUATION RESULTS (Using Embedding Similarity >= 0.90)\n",
      "================================================================================\n",
      "Evaluation samples: 5000\n",
      "Total topics in dataset: 76972\n",
      "\n",
      "Metrics:\n",
      "  F1 Score:            0.2072\n",
      "  Precision:           0.1671\n",
      "  Recall:              0.2726\n",
      "\n",
      "Prediction Stats:\n",
      "  Avg true topics:      1.84\n",
      "  Avg predicted topics: 3.00\n",
      "  Perfect matches:      711 (14.22%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL EVALUATION RESULTS (Using Embedding Similarity >= 0.90)\n",
      "================================================================================\n",
      "Evaluation samples: 5000\n",
      "Total topics in dataset: 76972\n",
      "\n",
      "Metrics:\n",
      "  F1 Score:            0.2072\n",
      "  Precision:           0.1671\n",
      "  Recall:              0.2726\n",
      "\n",
      "Prediction Stats:\n",
      "  Avg true topics:      1.84\n",
      "  Avg predicted topics: 3.00\n",
      "  Perfect matches:      711 (14.22%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detailed evaluation on larger sample\n",
    "DETAILED_EVAL_SIZE = min(5000, len(X_train))\n",
    "detailed_eval_indices = np.random.choice(len(X_train), DETAILED_EVAL_SIZE, replace=False)\n",
    "\n",
    "print(f\"Running detailed evaluation on {DETAILED_EVAL_SIZE} samples...\")\n",
    "y_pred_detailed = []\n",
    "y_true_detailed = [y_train[i] for i in detailed_eval_indices]\n",
    "\n",
    "for i in detailed_eval_indices:\n",
    "    pred_topics = predict_topics(\n",
    "        X_train[i],\n",
    "        topic_embeddings,\n",
    "        topic_ids_list,\n",
    "        min_score=0.3,\n",
    "        top_k=3\n",
    "    )\n",
    "    y_pred_detailed.append(pred_topics)\n",
    "\n",
    "# Calculate metrics using embedding similarity\n",
    "tp = 0  # True positives\n",
    "fp = 0  # False positives\n",
    "fn = 0  # False negatives\n",
    "perfect_matches = 0\n",
    "\n",
    "# Process each prediction\n",
    "for pred_topics, true_topics in zip(y_pred_detailed, y_true_detailed):\n",
    "    # Find matches based on embedding similarity\n",
    "    matched_pred, unmatched_pred = find_matching_topics(pred_topics, true_topics)\n",
    "\n",
    "    # Update metrics\n",
    "    tp += len(matched_pred)\n",
    "    fp += len(unmatched_pred)\n",
    "    # Count true topics that don't have a matching prediction\n",
    "    fn += len(true_topics) - len(matched_pred)\n",
    "\n",
    "    # Check for perfect match\n",
    "    if len(matched_pred) == len(true_topics):  # All true topics have matches\n",
    "        perfect_matches += 1\n",
    "\n",
    "# Calculate final metrics\n",
    "precision_final = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall_final = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_final = 2 * (precision_final * recall_final) / (precision_final + recall_final) if (precision_final + recall_final) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION RESULTS (Using Embedding Similarity >= 0.90)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Evaluation samples: {DETAILED_EVAL_SIZE}\")\n",
    "print(f\"Total topics in dataset: {len(topic_ids_list)}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  F1 Score:            {f1_final:.4f}\")\n",
    "print(f\"  Precision:           {precision_final:.4f}\")\n",
    "print(f\"  Recall:              {recall_final:.4f}\")\n",
    "print(f\"\\nPrediction Stats:\")\n",
    "print(f\"  Avg true topics:      {np.mean([len(t) for t in y_true_detailed]):.2f}\")\n",
    "print(f\"  Avg predicted topics: {np.mean([len(t) for t in y_pred_detailed]):.2f}\")\n",
    "print(f\"  Perfect matches:      {perfect_matches} ({perfect_matches/len(y_true_detailed)*100:.2f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Show Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREDICTION EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Example 1:\n",
      "================================================================================\n",
      "Content ID: c_acaf036f45b4\n",
      "Language: en\n",
      "\n",
      "Text: 8.5: Other Semi-Automated Approaches...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_dfafa322a6a7] 8.3: Non-linear Optimization: nan\n",
      "      Max Similarity: 0.3385 (✗ NO MATCH)\n",
      "    - [t_956f90e7870a] 8.1 Sampling techniques: nan\n",
      "      Max Similarity: 0.3839 (✗ NO MATCH)\n",
      "    - [t_a0286281c390] 8: Approximate Methods: nan\n",
      "      Max Similarity: 0.3928 (✗ NO MATCH)\n",
      "\n",
      "True Topics (1):\n",
      "    - [t_a1fb812ed7fb] 8: Bottom-up Ontology Development: 8: Bottom-up Ontology Development\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 0\n",
      "  - Unmatched predictions: 3\n",
      "  - Missing true topics: 1\n",
      "\n",
      "================================================================================\n",
      "Example 1:\n",
      "================================================================================\n",
      "Content ID: c_acaf036f45b4\n",
      "Language: en\n",
      "\n",
      "Text: 8.5: Other Semi-Automated Approaches...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_dfafa322a6a7] 8.3: Non-linear Optimization: nan\n",
      "      Max Similarity: 0.3385 (✗ NO MATCH)\n",
      "    - [t_956f90e7870a] 8.1 Sampling techniques: nan\n",
      "      Max Similarity: 0.3839 (✗ NO MATCH)\n",
      "    - [t_a0286281c390] 8: Approximate Methods: nan\n",
      "      Max Similarity: 0.3928 (✗ NO MATCH)\n",
      "\n",
      "True Topics (1):\n",
      "    - [t_a1fb812ed7fb] 8: Bottom-up Ontology Development: 8: Bottom-up Ontology Development\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 0\n",
      "  - Unmatched predictions: 3\n",
      "  - Missing true topics: 1\n",
      "\n",
      "================================================================================\n",
      "Example 2:\n",
      "================================================================================\n",
      "Content ID: c_4c82726b7bb0\n",
      "Language: es\n",
      "\n",
      "Text: Grade 6 Mathematics Module 4, Topic D, Lesson 12: Student Version Grade 6 Mathematics Module 4, Topic D, Lesson 12: Student Version (694.01 KB)...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_cb8e9106513b] Grade 6 Mathematics: nan\n",
      "      Max Similarity: 0.4675 (✗ NO MATCH)\n",
      "    - [t_278ab408535b] Grade 5 Mathematics: nan\n",
      "      Max Similarity: 0.4422 (✗ NO MATCH)\n",
      "    - [t_da5047b4c4be] Topic E: Lesson 29: Arithmetic series: nan\n",
      "      Max Similarity: 0.4937 (✗ NO MATCH)\n",
      "\n",
      "True Topics (1):\n",
      "    - [t_17a23702edd3] Lección 12: Resultados del estudiante Los estudiantes modelan y escriben expresiones equivalentes usando la prop\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 0\n",
      "  - Unmatched predictions: 3\n",
      "  - Missing true topics: 1\n",
      "\n",
      "================================================================================\n",
      "Example 2:\n",
      "================================================================================\n",
      "Content ID: c_4c82726b7bb0\n",
      "Language: es\n",
      "\n",
      "Text: Grade 6 Mathematics Module 4, Topic D, Lesson 12: Student Version Grade 6 Mathematics Module 4, Topic D, Lesson 12: Student Version (694.01 KB)...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_cb8e9106513b] Grade 6 Mathematics: nan\n",
      "      Max Similarity: 0.4675 (✗ NO MATCH)\n",
      "    - [t_278ab408535b] Grade 5 Mathematics: nan\n",
      "      Max Similarity: 0.4422 (✗ NO MATCH)\n",
      "    - [t_da5047b4c4be] Topic E: Lesson 29: Arithmetic series: nan\n",
      "      Max Similarity: 0.4937 (✗ NO MATCH)\n",
      "\n",
      "True Topics (1):\n",
      "    - [t_17a23702edd3] Lección 12: Resultados del estudiante Los estudiantes modelan y escriben expresiones equivalentes usando la prop\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 0\n",
      "  - Unmatched predictions: 3\n",
      "  - Missing true topics: 1\n",
      "\n",
      "================================================================================\n",
      "Example 3:\n",
      "================================================================================\n",
      "Content ID: c_c921d453b1c4\n",
      "Language: hi\n",
      "\n",
      "Text: वर्गीकरण करके द्विघातों का गुणनखंडन वर्गीकरण विधि का प्रयोग करते हुए 4y^2+4y-15 को (2y-3)(2y+5) के रूप में\n",
      "गुणनखंडित करें |...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_98a384f53f91] वर्गीकरण करके द्विघातों का गुणनखंडन: 1 के अलावा कोई और अग्रणी गुणांक वाले द्विघात व्यंजकों का गुणनखंडन करना सीखें। उदाहरण के लिए, 2x²+7x+\n",
      "      Max Similarity: 1.0000 (✓ MATCH)\n",
      "    - [t_9cec5822a942] बहुपदों को जोड़ना और घटाना (भाग2/2): दो बहुपदों को जोड़कर या घटाकर एक अन्य बहुपद ज्ञात करना सीखें। उदाहरण के लिए,  (3x^3+2x-1)+(2x^4-x^3+7\n",
      "      Max Similarity: 0.8372 (✗ NO MATCH)\n",
      "    - [t_9908120beee7] बीजीय व्यंजक को जोड़ना और घटाना: दो चर: सीखिए की कैसे दो चरों वाले बहुपदों को जोड़ा और घटाया जा सकता है। उदाहरण के लिए, x^3 + xy + 3y - (x^3 \n",
      "      Max Similarity: 0.7603 (✗ NO MATCH)\n",
      "\n",
      "True Topics (2):\n",
      "    - [t_98a384f53f91] वर्गीकरण करके द्विघातों का गुणनखंडन: 1 के अलावा कोई और अग्रणी गुणांक वाले द्विघात व्यंजकों का गुणनखंडन करना सीखें। उदाहरण के लिए, 2x²+7x+\n",
      "    - [t_f10952bf7b07] समूहन करके द्विघातों का गुणनखंडन: nan\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 1\n",
      "  - Unmatched predictions: 2\n",
      "  - Missing true topics: 1\n",
      "\n",
      "================================================================================\n",
      "Example 3:\n",
      "================================================================================\n",
      "Content ID: c_c921d453b1c4\n",
      "Language: hi\n",
      "\n",
      "Text: वर्गीकरण करके द्विघातों का गुणनखंडन वर्गीकरण विधि का प्रयोग करते हुए 4y^2+4y-15 को (2y-3)(2y+5) के रूप में\n",
      "गुणनखंडित करें |...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_98a384f53f91] वर्गीकरण करके द्विघातों का गुणनखंडन: 1 के अलावा कोई और अग्रणी गुणांक वाले द्विघात व्यंजकों का गुणनखंडन करना सीखें। उदाहरण के लिए, 2x²+7x+\n",
      "      Max Similarity: 1.0000 (✓ MATCH)\n",
      "    - [t_9cec5822a942] बहुपदों को जोड़ना और घटाना (भाग2/2): दो बहुपदों को जोड़कर या घटाकर एक अन्य बहुपद ज्ञात करना सीखें। उदाहरण के लिए,  (3x^3+2x-1)+(2x^4-x^3+7\n",
      "      Max Similarity: 0.8372 (✗ NO MATCH)\n",
      "    - [t_9908120beee7] बीजीय व्यंजक को जोड़ना और घटाना: दो चर: सीखिए की कैसे दो चरों वाले बहुपदों को जोड़ा और घटाया जा सकता है। उदाहरण के लिए, x^3 + xy + 3y - (x^3 \n",
      "      Max Similarity: 0.7603 (✗ NO MATCH)\n",
      "\n",
      "True Topics (2):\n",
      "    - [t_98a384f53f91] वर्गीकरण करके द्विघातों का गुणनखंडन: 1 के अलावा कोई और अग्रणी गुणांक वाले द्विघात व्यंजकों का गुणनखंडन करना सीखें। उदाहरण के लिए, 2x²+7x+\n",
      "    - [t_f10952bf7b07] समूहन करके द्विघातों का गुणनखंडन: nan\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 1\n",
      "  - Unmatched predictions: 2\n",
      "  - Missing true topics: 1\n",
      "\n",
      "================================================================================\n",
      "Example 4:\n",
      "================================================================================\n",
      "Content ID: c_5d9a858e4f6e\n",
      "Language: es\n",
      "\n",
      "Text: Adición de Enteros...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_8cbfb359f18d] Uso de Enteros: nan\n",
      "      Max Similarity: 0.3358 (✗ NO MATCH)\n",
      "    - [t_8d19a9549ec4] Addition: nan\n",
      "      Max Similarity: 0.3894 (✗ NO MATCH)\n",
      "    - [t_bc822bad2b2b] Addition: nan\n",
      "      Max Similarity: 0.3894 (✗ NO MATCH)\n",
      "\n",
      "True Topics (1):\n",
      "    - [t_41f2dcf0aa55] Propiedades de los Números Reales: nan\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 0\n",
      "  - Unmatched predictions: 3\n",
      "  - Missing true topics: 1\n",
      "\n",
      "================================================================================\n",
      "Example 4:\n",
      "================================================================================\n",
      "Content ID: c_5d9a858e4f6e\n",
      "Language: es\n",
      "\n",
      "Text: Adición de Enteros...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_8cbfb359f18d] Uso de Enteros: nan\n",
      "      Max Similarity: 0.3358 (✗ NO MATCH)\n",
      "    - [t_8d19a9549ec4] Addition: nan\n",
      "      Max Similarity: 0.3894 (✗ NO MATCH)\n",
      "    - [t_bc822bad2b2b] Addition: nan\n",
      "      Max Similarity: 0.3894 (✗ NO MATCH)\n",
      "\n",
      "True Topics (1):\n",
      "    - [t_41f2dcf0aa55] Propiedades de los Números Reales: nan\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 0\n",
      "  - Unmatched predictions: 3\n",
      "  - Missing true topics: 1\n",
      "\n",
      "================================================================================\n",
      "Example 5:\n",
      "================================================================================\n",
      "Content ID: c_b200f3d85c63\n",
      "Language: zh\n",
      "\n",
      "Text: 独特市场价值(UVP)表格(PDF)...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_bb4193bd9fc7] سوقُ \"الأَنْتيكات\": nan\n",
      "      Max Similarity: 0.1964 (✗ NO MATCH)\n",
      "    - [t_81d4a6cd38f0] سوقُ \"الأَنْتيكات\": nan\n",
      "      Max Similarity: 0.1964 (✗ NO MATCH)\n",
      "    - [t_57ee39f64912] आधारभूत त्रिकोणमितीय व्यंजकों का मूल्यांकन: nan\n",
      "      Max Similarity: 0.1742 (✗ NO MATCH)\n",
      "\n",
      "True Topics (1):\n",
      "    - [t_cc618ba45c9b] 可下載的資源: nan\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 0\n",
      "  - Unmatched predictions: 3\n",
      "  - Missing true topics: 1\n",
      "\n",
      "================================================================================\n",
      "Example 5:\n",
      "================================================================================\n",
      "Content ID: c_b200f3d85c63\n",
      "Language: zh\n",
      "\n",
      "Text: 独特市场价值(UVP)表格(PDF)...\n",
      "\n",
      "Predicted Topics (3):\n",
      "    - [t_bb4193bd9fc7] سوقُ \"الأَنْتيكات\": nan\n",
      "      Max Similarity: 0.1964 (✗ NO MATCH)\n",
      "    - [t_81d4a6cd38f0] سوقُ \"الأَنْتيكات\": nan\n",
      "      Max Similarity: 0.1964 (✗ NO MATCH)\n",
      "    - [t_57ee39f64912] आधारभूत त्रिकोणमितीय व्यंजकों का मूल्यांकन: nan\n",
      "      Max Similarity: 0.1742 (✗ NO MATCH)\n",
      "\n",
      "True Topics (1):\n",
      "    - [t_cc618ba45c9b] 可下載的資源: nan\n",
      "\n",
      "Matches Summary:\n",
      "  - Matched topics: 0\n",
      "  - Unmatched predictions: 3\n",
      "  - Missing true topics: 1\n"
     ]
    }
   ],
   "source": [
    "# Show prediction examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_topic_similarities(pred_topics, true_topics):\n",
    "    \"\"\"Get similarity scores between predicted and true topics.\"\"\"\n",
    "    pred_texts = [topics_df[topics_df['id'] == tid]['combined_text'].iloc[0] for tid in pred_topics]\n",
    "    true_texts = [topics_df[topics_df['id'] == tid]['combined_text'].iloc[0] for tid in true_topics]\n",
    "\n",
    "    pred_embeddings = model.encode(pred_texts)\n",
    "    true_embeddings = model.encode(true_texts)\n",
    "\n",
    "    similarities = cosine_similarity(pred_embeddings, true_embeddings)\n",
    "    return similarities\n",
    "\n",
    "num_examples = min(5, len(detailed_eval_indices))\n",
    "for i in range(num_examples):\n",
    "    idx = detailed_eval_indices[i]\n",
    "    content_row = content_df_sampled.iloc[train_indices[idx]]\n",
    "\n",
    "    pred_topics = y_pred_detailed[i]\n",
    "    true_topics = y_true_detailed[i]\n",
    "\n",
    "    # Get similarity matrix\n",
    "    similarities = get_topic_similarities(pred_topics, true_topics)\n",
    "    matched_pred, unmatched_pred = find_matching_topics(pred_topics, true_topics)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Content ID: {content_row['id']}\")\n",
    "    print(f\"Language: {content_row.get('language', 'N/A')}\")\n",
    "    print(f\"\\nText: {content_row['combined_text'][:200]}...\")\n",
    "\n",
    "    print(f\"\\nPredicted Topics ({len(pred_topics)}):\")\n",
    "    for j, tid in enumerate(pred_topics):\n",
    "        topic_row = topics_df[topics_df['id'] == tid]\n",
    "        if not topic_row.empty:\n",
    "            title = topic_row.iloc[0]['title']\n",
    "            desc = str(topic_row.iloc[0]['description'])[:100]\n",
    "            max_sim = similarities[j].max() if len(similarities[j]) > 0 else 0\n",
    "            match_status = \"✓ MATCH\" if tid in matched_pred else \"✗ NO MATCH\"\n",
    "            print(f\"    - [{tid}] {title}: {desc}\")\n",
    "            print(f\"      Max Similarity: {max_sim:.4f} ({match_status})\")\n",
    "\n",
    "    print(f\"\\nTrue Topics ({len(true_topics)}):\")\n",
    "    get_topic_details(true_topics)\n",
    "\n",
    "    print(f\"\\nMatches Summary:\")\n",
    "    print(f\"  - Matched topics: {len(matched_pred)}\")\n",
    "    print(f\"  - Unmatched predictions: {len(unmatched_pred)}\")\n",
    "    print(f\"  - Missing true topics: {len(true_topics) - len(matched_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL SAVED\n",
      "================================================================================\n",
      "Location: /Users/pablowatfi/repos/topic-pred/artifacts/topic_predictor_direct_model.pkl\n",
      "\n",
      "Model includes:\n",
      "  - 76972 topic embeddings\n",
      "  - Topic metadata and mappings\n",
      "  - Performance metrics (F1=0.2072)\n",
      "  - Training configuration\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save model artifacts\n",
    "model_artifacts = {\n",
    "    'topic_embeddings': topic_embeddings,\n",
    "    'topic_ids_list': topic_ids_list,\n",
    "}\n",
    "\n",
    "output_path = ARTIFACTS_DIR / 'topic_predictor_direct_model.pkl'\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "joblib.dump(model_artifacts, output_path)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL SAVED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Location: {output_path}\")\n",
    "print(f\"\\nModel includes:\")\n",
    "print(f\"  - {len(topic_ids_list)} topic embeddings\")\n",
    "print(f\"  - Topic metadata and mappings\")\n",
    "print(f\"  - Performance metrics (F1={f1_final:.4f})\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
